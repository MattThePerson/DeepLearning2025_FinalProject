{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c965cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "import sys\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd049986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# region Data Sets/Loaders\n",
    "# ========================\n",
    "class RetinaMultiLabelDataset(Dataset):\n",
    "    def __init__(self, csv_file, image_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        img_path = os.path.join(self.image_dir, row.iloc[0])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        labels = torch.tensor(row[1:].values.astype(\"float32\"))\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, labels\n",
    "\n",
    "class RetinaMultiLabelDataset_WithoutLabels(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.images = os.listdir(image_dir)\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, img_name\n",
    "\n",
    "\n",
    "def get_dataloaders(dataset_path: str, img_size=256, batch_size=32):\n",
    "    \n",
    "    # transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "    ])\n",
    "\n",
    "    # paths\n",
    "    train_csv = os.path.join( dataset_path, \"labels/train.csv\" )\n",
    "    val_csv   = os.path.join( dataset_path, \"labels/val.csv\" )\n",
    "    test_csv  = os.path.join( dataset_path, \"labels/offsite_test.csv\" )\n",
    "    \n",
    "    train_image_dir = os.path.join( dataset_path, \"images/train\" )\n",
    "    val_image_dir =   os.path.join( dataset_path, \"images/val\" )\n",
    "    test_image_dir =  os.path.join( dataset_path, \"images/offsite_test\" )\n",
    "    onsite_test_image_dir =  os.path.join( dataset_path, \"images/onsite_test\" )\n",
    "\n",
    "    # dataset & dataloader\n",
    "    train_ds =       RetinaMultiLabelDataset(train_csv, train_image_dir, transform)\n",
    "    val_ds   =       RetinaMultiLabelDataset(val_csv, val_image_dir, transform)\n",
    "    test_ds  =       RetinaMultiLabelDataset(test_csv, test_image_dir, transform)\n",
    "    onsite_test_ds = RetinaMultiLabelDataset_WithoutLabels(onsite_test_image_dir, transform)\n",
    "\n",
    "    train_loader =        DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader   =        DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    test_loader  =        DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    onsite_test_loader  = DataLoader(onsite_test_ds, batch_size=1, shuffle=False, num_workers=4)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, onsite_test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e6225cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# region BUILD MODEL\n",
    "# ========================\n",
    "\n",
    "def build_model(backbone=\"resnet18\", num_classes=3):\n",
    "\n",
    "    if backbone == \"resnet18\":\n",
    "        model = models.resnet18(weights=None) #\"IMAGENET1K_V1\") # TODO: should this be None??\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    elif backbone == \"efficientnet\":\n",
    "        model = models.efficientnet_b0(weights=None) #\"IMAGENET1K_V1\")\n",
    "        layer_fc: nn.Linear = model.classifier[1] # type: ignore[assignment]\n",
    "        model.classifier[1] = nn.Linear(layer_fc.in_features, num_classes)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported backbone: {backbone}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model(backbone=\"resnet18\", pretrained_params=None, freeze_backbone=False, num_classes=3):\n",
    "    \n",
    "    model = build_model(backbone, num_classes)\n",
    "    \n",
    "    # parameters freezing\n",
    "    if freeze_backbone:\n",
    "        print('FREEZING: freezing model backbone (non-Linear layers)')\n",
    "        model = freeze_non_linear_layers(model)\n",
    "    \n",
    "    else:\n",
    "        print('FREEZING: Unfreezing all layers')\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = True\n",
    "    \n",
    "    # pretrained params\n",
    "    if pretrained_params is not None:\n",
    "        state_dict = torch.load(pretrained_params, map_location=\"cpu\")\n",
    "        try:\n",
    "            model.load_state_dict(state_dict)\n",
    "        except:\n",
    "            print(f\"ERROR: Incompatible backbone ({backbone}) and params file ({pretrained_params})\\nexiting ...\")\n",
    "            sys.exit(2)\n",
    "    \n",
    "    # print param amounts\n",
    "    all_params, trainable_params = get_parameter_count(model)\n",
    "    print('=====================')\n",
    "    print('    LOADED MODEL')\n",
    "    print('---------------------')\n",
    "    print('backbone:', backbone)\n",
    "    print('pretrained params:', pretrained_params)\n",
    "    print('parameter count:  {:_d}'.format(all_params))\n",
    "    print('trainable params: {:_d}'.format(trainable_params))\n",
    "    print('frozen params:    {:_d}'.format(all_params-trainable_params))\n",
    "    print('=====================')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd6315c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# region HELPERS\n",
    "# ========================\n",
    "\n",
    "def freeze_non_linear_layers(model):\n",
    "    \"\"\"\n",
    "    Freeze backbone and leave classifier (linear layers) unfrozen. \n",
    "    \"\"\"\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    # Unfreeze only Linear layers\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            for p in m.parameters():\n",
    "                p.requires_grad = True\n",
    "    return model\n",
    "\n",
    "def get_parameter_count(model): \n",
    "    all_params =       sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return all_params, trainable_params\n",
    "\n",
    "def ensure_parent_exists(file: str):\n",
    "    os.makedirs(os.path.dirname(file), exist_ok=True)\n",
    "\n",
    "# ========================\n",
    "# region CUSTOM LOSS\n",
    "# ========================\n",
    "\n",
    "def FocalLoss(x):\n",
    "    \"\"\"\n",
    "    Focal Loss: A loss function designed to address class imbalance by downweighting easy examples and focusing\n",
    "    training on hard, misclassified ones.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def ClassBalancedLoss(x):\n",
    "    \"\"\"\n",
    "    Class-Balanced Loss: Re-weight the BCE loss according to class frequency. This is a common method for handling\n",
    "    class imbalance.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6123a9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# region predict\n",
    "# ========================\n",
    "def predict(\n",
    "        model: nn.Module,\n",
    "        loader: DataLoader,\n",
    "        csv_path=\"onsite_test_submission.csv\",\n",
    "    ):\n",
    "    \n",
    "    model.eval()\n",
    "    data = []\n",
    "    print(f'generating predictions for {len(loader.dataset)} images') # type: ignore\n",
    "    with torch.no_grad():\n",
    "        for img, img_name in tqdm(loader):\n",
    "            img_name = img_name[0]\n",
    "            img = img.to(DEVICE)\n",
    "            output = model(img)[0]\n",
    "            probs = torch.sigmoid(output).cpu().numpy()\n",
    "            preds = (probs > 0.5).astype(int)\n",
    "            data_line = [img_name]\n",
    "            data_line.extend(preds)\n",
    "            data.append(data_line)\n",
    "    \n",
    "    # write to csv\n",
    "    if not csv_path.endswith(\".csv\"): csv_path += \".csv\"\n",
    "    ensure_parent_exists(csv_path)\n",
    "    print(f'writing predictions to {csv_path}')\n",
    "    with open(csv_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"id\",\"D\",\"G\",\"A\"])\n",
    "        writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5791907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# region test\n",
    "# ========================\n",
    "def test(\n",
    "        model: nn.Module,\n",
    "        loader: DataLoader,\n",
    "    ):\n",
    "\n",
    "    print(f'Testing model on {len(loader.dataset)} images') # type: ignore\n",
    "\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(loader, colour=\"magenta\"):\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            outputs = model(imgs)\n",
    "            probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "            preds = (probs > 0.5).astype(int)\n",
    "            y_true.extend(labels.numpy())\n",
    "            y_pred.extend(preds)\n",
    "\n",
    "    y_true = np.array(y_true) #torch.tensor(y_true).numpy()\n",
    "    y_pred = np.array(y_pred) #torch.tensor(y_pred).numpy()\n",
    "\n",
    "    # compute metrics\n",
    "    disease_names = [\"DR\", \"Glaucoma\", \"AMD\"]\n",
    "    results_data = []\n",
    "    disease_counts = []\n",
    "    \n",
    "    for i, disease in enumerate(disease_names):  # compute metrics for every disease\n",
    "        y_t = y_true[:, i]\n",
    "        y_p = y_pred[:, i]\n",
    "\n",
    "        disease_counts.append(y_t.sum())\n",
    "        acc =       accuracy_score(y_t, y_p)\n",
    "        precision = precision_score(y_t, y_p, average=\"macro\", zero_division=0)\n",
    "        recall =    recall_score(y_t, y_p, average=\"macro\", zero_division=0)\n",
    "        f1 =        f1_score(y_t, y_p, average=\"macro\", zero_division=0)\n",
    "        kappa =     cohen_kappa_score(y_t, y_p)\n",
    "\n",
    "        results_data.append([disease, acc, precision, recall, f1, kappa])\n",
    "\n",
    "    disease_count = np.array(disease_counts)\n",
    "    results = pd.DataFrame(\n",
    "        data=results_data, \n",
    "        columns=[\"Disease\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-score\", \"Kappa\"],\n",
    "    ).set_index(\"Disease\")\n",
    "\n",
    "    results = results.T\n",
    "    print(results.values)\n",
    "    results[\"SUM\"] = results.values.sum(axis=1)\n",
    "    results[\"MEAN\"] = results.values.sum(axis=1) / 3\n",
    "    results[\"WEIGHTEDMEAN\"] = results.values.sum(axis=1) * disease_count\n",
    "    print(\"========================\")\n",
    "    print(\"DISEASE SPECIFIC METRICS:\\n\")\n",
    "    print(results.T)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d7a81f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# region train\n",
    "# ========================\n",
    "def train(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        optimizer,\n",
    "        loss_fn,\n",
    "        epochs=10,\n",
    "        save_name=\"checkpoints/best.pt\",\n",
    "    ):\n",
    "\n",
    "    ensure_parent_exists(save_name)\n",
    "    print('loss function:', loss_fn)\n",
    "    \n",
    "    # iterates\n",
    "    best_val_loss = float(\"inf\")\n",
    "    for epoch in range(epochs):\n",
    "        # print('training ...')\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for imgs, labels in tqdm(train_loader, desc=f\"epoch {epoch+1}/{epochs}\", colour=\"purple\"):\n",
    "            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * imgs.size(0)\n",
    "        train_loss /= len(train_loader.dataset) # type: ignore\n",
    "\n",
    "        # validation\n",
    "        # print('evaluating ...')\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in val_loader:\n",
    "                imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(imgs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                val_loss += loss.item() * imgs.size(0)\n",
    "        val_loss /= len(val_loader.dataset) # type: ignore\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # save best\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            print(f\"    ..saving best checkpoint to: {save_name}\")\n",
    "            torch.save(model.state_dict(), save_name)\n",
    "\n",
    "    return save_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a56fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# region MAIN\n",
    "# ========================\n",
    "def main(\n",
    "        mode = \"train\",\n",
    "        backbone = \"resnet18\",\n",
    "        dataset_path = \"dataset\",\n",
    "        pretrained_params: str|None = None,\n",
    "        save_name: str|None = None,\n",
    "        freeze_backbone = True, # freeze non-linear layers\n",
    "        loss_fn = nn.BCEWithLogitsLoss,\n",
    "        attention = None,\n",
    "        predict_csv = \"onsite_test_submission.csv\",\n",
    "        # save_dir=\"checkpoints\",\n",
    "        epochs=10, batch_size=32, lr=1e-4, img_size=256,\n",
    "    ):\n",
    "    \n",
    "    # MODE\n",
    "    match mode:\n",
    "        case \"train\": # - train -------\n",
    "            optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "\n",
    "            print(f'Training {backbone} for {epochs} epochs')\n",
    "            train(\n",
    "                model,\n",
    "                train_loader,\n",
    "                val_loader,\n",
    "                optimizer,\n",
    "                loss_fn,\n",
    "                epochs=epochs,\n",
    "                save_name=save_name,\n",
    "            )\n",
    "            test(\n",
    "                model,\n",
    "                test_loader,\n",
    "            )\n",
    "        \n",
    "        case \"test\":  # - test ---------\n",
    "            test(\n",
    "                model,\n",
    "                test_loader,\n",
    "            )\n",
    "        \n",
    "        case \"predict\": # - predict ---\n",
    "            print(\"Predicting ...\")\n",
    "            predict(\n",
    "                model,\n",
    "                onsite_test_loader,\n",
    "                csv_path=predict_csv,\n",
    "            )\n",
    "        \n",
    "        case \"none\": # - none ---------\n",
    "            print(\"Just a test, nothing to see here\")\n",
    "        \n",
    "        case _:\n",
    "            print(\"oh no, no mode is matched??\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c93f923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ========================\n",
    "# region CLI\n",
    "# ========================\n",
    "\n",
    "LOSS_FUNCS = {\n",
    "    \"bce\": nn.BCEWithLogitsLoss(),\n",
    "    \"focal\": FocalLoss,\n",
    "    \"class_balanced\": ClassBalancedLoss,\n",
    "}\n",
    "\n",
    "DEVICE: torch.device\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('using device:', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1368a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n",
      "FREEZING: freezing model backbone (non-Linear layers)\n",
      "=====================\n",
      "    LOADED MODEL\n",
      "---------------------\n",
      "backbone: resnet18\n",
      "pretrained params: checkpoints/best_resnet18.pt\n",
      "parameter count:  11_178_051\n",
      "trainable params: 1_539\n",
      "frozen params:    11_176_512\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader, onsite_test_loader = get_dataloaders(\"ODIR_dataset\", 256, 32)\n",
    "\n",
    "print('Building model')\n",
    "model = get_model(\n",
    "    backbone = \"resnet18\",\n",
    "    pretrained_params = \"checkpoints/best_resnet18.pt\",\n",
    "    freeze_backbone = True,\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85701b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[35m██████████\u001b[0m| 7/7 [00:22<00:00,  3.16s/it]\n"
     ]
    }
   ],
   "source": [
    "# get metrics\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in tqdm(test_loader, colour=\"magenta\"):\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        outputs = model(imgs)\n",
    "        probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "        preds = (probs > 0.5).astype(int)\n",
    "        y_true.extend(labels.numpy())\n",
    "        y_pred.extend(preds)\n",
    "\n",
    "y_true = np.array(y_true) #torch.tensor(y_true).numpy()\n",
    "y_pred = np.array(y_pred) #torch.tensor(y_pred).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb0ce24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metrics\n",
    "disease_names = [\"DR\", \"Glaucoma\", \"AMD\"]\n",
    "results_data = []\n",
    "\n",
    "for i, disease in enumerate(disease_names):  # compute metrics for every disease\n",
    "    y_t = y_true[:, i]\n",
    "    y_p = y_pred[:, i]\n",
    "\n",
    "    acc =       accuracy_score(y_t, y_p)\n",
    "    precision = precision_score(y_t, y_p, average=\"macro\", zero_division=0)\n",
    "    recall =    recall_score(y_t, y_p, average=\"macro\", zero_division=0)\n",
    "    f1 =        f1_score(y_t, y_p, average=\"macro\", zero_division=0)\n",
    "    kappa =     cohen_kappa_score(y_t, y_p)\n",
    "\n",
    "    results_data.append([disease, acc, precision, recall, f1, kappa])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b4c72f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         METRIC | F1-SCORE\n",
      "             DC : 0.76800\n",
      "            Gla : 0.61728\n",
      "            AMD : 0.48276\n",
      "        average : 0.70326\n",
      "       weighted : 0.70326\n",
      "           mean : 0.62268\n",
      "          ravel : 0.69409\n",
      "           full : 0.62268\n"
     ]
    }
   ],
   "source": [
    "d = {}\n",
    "f1_scores = [ f1_score(y_true[:,i], y_pred[:,i], average=\"binary\", zero_division=0) for i in range(3) ]\n",
    "d[\"DC\"], d[\"Gla\"], d[\"AMD\"] = f1_scores\n",
    "d[\"average\"] = np.average(f1_scores, weights=y_true.sum(axis=0))\n",
    "d[\"weighted\"] = (f1_score(y_true, y_pred, average=\"weighted\", zero_division=0))\n",
    "d[\"mean\"] = np.average(f1_scores)\n",
    "d[\"ravel\"] = f1_score(y_true.ravel(), y_pred.ravel(), average=\"binary\", zero_division=0)\n",
    "d[\"full\"] = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "print('{:>15} | {}'.format(\"METRIC\", \"F1-SCORE\"))\n",
    "for k, v in d.items():\n",
    "    print(f'{k:>15} : {v:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7823eaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"notebooks/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aed432a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(\n",
    "    data=results_data, \n",
    "    columns=[\"Category\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-score\", \"Kappa\"],\n",
    ").set_index(\"Category\")\n",
    "\n",
    "results = results.T\n",
    "disease_occurances = y_true.sum(axis=0)\n",
    "results[\"Average\"] = np.average(results.values, axis=1, weights=disease_occurances)\n",
    "results = results.T\n",
    "results.to_csv(\"notebooks/test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
